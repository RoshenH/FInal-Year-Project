{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e79d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "X_tensor shape: torch.Size([1088, 2000])\n",
      "y_tensor shape: torch.Size([1088])\n",
      "Epoch 1/30 - Triplet Loss: 1.0581, Samples: 1088\n",
      "Epoch 2/30 - Triplet Loss: 1.0298, Samples: 1088\n",
      "Epoch 3/30 - Triplet Loss: 1.0199, Samples: 1088\n",
      "Epoch 4/30 - Triplet Loss: 0.9972, Samples: 1088\n",
      "Epoch 5/30 - Triplet Loss: 1.0222, Samples: 1088\n",
      "Epoch 6/30 - Triplet Loss: 1.0056, Samples: 1088\n",
      "Epoch 7/30 - Triplet Loss: 1.0120, Samples: 1088\n",
      "Epoch 8/30 - Triplet Loss: 0.9987, Samples: 1088\n",
      "Epoch 9/30 - Triplet Loss: 1.0164, Samples: 1088\n",
      "Epoch 10/30 - Triplet Loss: 1.0010, Samples: 1088\n",
      "Epoch 11/30 - Triplet Loss: 0.9990, Samples: 1088\n",
      "Epoch 12/30 - Triplet Loss: 1.0001, Samples: 1088\n",
      "Epoch 13/30 - Triplet Loss: 1.0052, Samples: 1088\n",
      "Epoch 14/30 - Triplet Loss: 1.0074, Samples: 1088\n",
      "Epoch 15/30 - Triplet Loss: 0.9917, Samples: 1088\n",
      "Epoch 16/30 - Triplet Loss: 1.0148, Samples: 1088\n",
      "Epoch 17/30 - Triplet Loss: 0.9967, Samples: 1088\n",
      "Epoch 18/30 - Triplet Loss: 0.9988, Samples: 1088\n",
      "Epoch 19/30 - Triplet Loss: 0.9961, Samples: 1088\n",
      "Epoch 20/30 - Triplet Loss: 0.9913, Samples: 1088\n",
      "Epoch 21/30 - Triplet Loss: 1.0141, Samples: 1088\n",
      "Epoch 22/30 - Triplet Loss: 0.9976, Samples: 1088\n",
      "Epoch 23/30 - Triplet Loss: 0.9829, Samples: 1088\n",
      "Epoch 24/30 - Triplet Loss: 1.0057, Samples: 1088\n",
      "Epoch 25/30 - Triplet Loss: 0.9960, Samples: 1088\n",
      "Epoch 26/30 - Triplet Loss: 1.0099, Samples: 1088\n",
      "Epoch 27/30 - Triplet Loss: 1.0033, Samples: 1088\n",
      "Epoch 28/30 - Triplet Loss: 1.0084, Samples: 1088\n",
      "Epoch 29/30 - Triplet Loss: 0.9867, Samples: 1088\n",
      "Epoch 30/30 - Triplet Loss: 0.9959, Samples: 1088\n",
      "Generating final embeddings...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loader_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 190\u001b[0m\n\u001b[0;32m    188\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mloader_full\u001b[49m:\n\u001b[0;32m    191\u001b[0m         X_batch \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    192\u001b[0m         emb \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(X_batch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loader_full' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Data\n",
    "X_flux = np.load(\"X_flux_aligned.npy\")\n",
    "y = np.load(\"y.npy\")\n",
    "\n",
    "X_flux_full = np.load(\"X_flux_aligned.npy\")  # full dataset\n",
    "X_tensor = torch.tensor(X_flux_full, dtype=torch.float32)\n",
    "\n",
    "X_tensor = torch.tensor(X_flux, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "print(f\"X_tensor shape: {X_tensor.shape}\")\n",
    "print(f\"y_tensor shape: {y_tensor.shape}\")\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Custom Multi-Head Attention\n",
    "class CustomMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "\n",
    "        output = self.out_linear(context)\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = CustomMultiHeadAttention(d_model, nhead, dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2, _ = self.self_attn(src, src, src)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(torch.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        return self.norm2(src)\n",
    "\n",
    "# Autoencoder with encoder-only output\n",
    "class FluxTransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2, seq_len=2000):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, seq_len)\n",
    "        self.encoder_blocks = nn.Sequential(*[TransformerEncoderBlock(d_model, nhead) for _ in range(num_layers)])\n",
    "        self.decoder = nn.Linear(d_model, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = self.input_projection(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.decoder(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = self.input_projection(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        return x[:, 0, :]  # First token as embedding\n",
    "\n",
    "# Hard Triplet Loss\n",
    "def batch_hard_triplet_mining(embeddings, labels, margin=1.0):\n",
    "    dist_matrix = torch.cdist(embeddings, embeddings, p=2)\n",
    "    labels = labels.unsqueeze(1)\n",
    "    matches = labels == labels.T\n",
    "    diffs = ~matches\n",
    "    hardest_pos = torch.where(matches, dist_matrix, torch.tensor(float('-inf')).to(embeddings.device)).max(dim=1)[0]\n",
    "    hardest_neg = torch.where(diffs, dist_matrix, torch.tensor(float('inf')).to(embeddings.device)).min(dim=1)[0]\n",
    "    triplet_loss = F.relu(hardest_pos - hardest_neg + margin)\n",
    "    return triplet_loss.mean()\n",
    "\n",
    "# Training setup\n",
    "model = FluxTransformerAutoencoder().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 30\n",
    "batch_size = 8\n",
    "virtual_batch_size = 32  # effective batch size\n",
    "accum_steps = virtual_batch_size // batch_size\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=batch_size, shuffle=True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (X_batch, y_batch) in enumerate(loader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        embeddings = model.encode(X_batch)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        loss = batch_hard_triplet_mining(embeddings, y_batch)\n",
    "        loss = loss / accum_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accum_steps == 0 or (i + 1) == len(loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * batch_size * accum_steps\n",
    "        total_samples += batch_size\n",
    "\n",
    "        # ðŸ§¹ Optional: cleanup to help long loops\n",
    "        del X_batch, y_batch, embeddings, loss\n",
    "        torch.cuda.empty_cache() if device.type == \"cuda\" else None\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Triplet Loss: {avg_loss:.4f}, Samples: {total_samples}\")\n",
    "\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating final embeddings...\")\n",
    "model.eval()\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for batch in loader_full:\n",
    "        X_batch = batch[0].to(device)\n",
    "        emb = model.encode(X_batch)\n",
    "        embeddings.append(emb.cpu().numpy())\n",
    "        del X_batch, emb\n",
    "        torch.cuda.empty_cache() if device.type == \"cuda\" else None\n",
    "flux_embeddings = np.concatenate(embeddings, axis=0)\n",
    "print(\"âœ… flux_embeddings shape:\", flux_embeddings.shape)\n",
    "\n",
    "# Save embeddings\n",
    "np.save(\"flux_embeddings.npy\", flux_embeddings)\n",
    "\n",
    "# UMAP Visualization\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "embedding_umap = reducer.fit_transform(flux_embeddings)\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(embedding_umap[:, 0], embedding_umap[:, 1], c=y, cmap='viridis')\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"UMAP Visualization of Flux Embeddings\")\n",
    "plt.show()\n",
    "\n",
    "# t-SNE Visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embedding_tsne = tsne.fit_transform(flux_embeddings)\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(embedding_tsne[:, 0], embedding_tsne[:, 1], c=y, cmap='viridis')\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"t-SNE Visualization of Flux Embeddings\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08fa5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mloader_full\u001b[49m:\n\u001b[0;32m      6\u001b[0m         X_batch \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m         emb \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(X_batch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loader_full' is not defined"
     ]
    }
   ],
   "source": [
    "np.save(\"flux_embeddings.npy\", flux_embeddings)\n",
    "print(f\"âœ… flux_embeddings.npy saved: {flux_embeddings.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
