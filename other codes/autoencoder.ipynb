{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42a56442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded X_flux_aligned.npy: (1088, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 68/68 [10:07<00:00,  8.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 Loss: 0.004037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 68/68 [08:55<00:00,  7.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 Loss: 0.000244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 68/68 [09:17<00:00,  8.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 Loss: 0.000107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 68/68 [10:32<00:00,  9.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 Loss: 0.000051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 68/68 [10:40<00:00,  9.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 Loss: 0.000024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 68/68 [08:28<00:00,  7.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 Loss: 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 68/68 [08:00<00:00,  7.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 Loss: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 68/68 [09:07<00:00,  8.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 Loss: 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 68/68 [08:30<00:00,  7.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9 Loss: 0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 68/68 [08:24<00:00,  7.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10 Loss: 0.000003\n",
      "\n",
      "🔍 Extracting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 68/68 [00:54<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ flux_embeddings.npy saved: (1088, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# ----------------------------------\n",
    "# Load Flux Data\n",
    "# ----------------------------------\n",
    "X_flux = np.load(\"X_flux_aligned.npy\")\n",
    "print(\"✅ Loaded X_flux_aligned.npy:\", X_flux.shape)  # (samples, 2000)\n",
    "\n",
    "# Add channel dimension: (samples, 2000, 1)\n",
    "if len(X_flux.shape) == 2:\n",
    "    X_flux = X_flux[..., np.newaxis]\n",
    "\n",
    "X_tensor = torch.tensor(X_flux, dtype=torch.float32)\n",
    "dataset = TensorDataset(X_tensor)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# ----------------------------------\n",
    "# Positional Encoding\n",
    "# ----------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# ----------------------------------\n",
    "# Transformer Encoder Block\n",
    "# ----------------------------------\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=64, nhead=4, dim_feedforward=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                        dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch, d_model)\n",
    "        out = self.transformer_encoder(x)\n",
    "        return out.permute(1, 0, 2)  # (batch, seq_len, d_model)\n",
    "\n",
    "# ----------------------------------\n",
    "# Optimized Autoencoder Model\n",
    "# ----------------------------------\n",
    "class FluxTransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len=2000, d_model=64):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(1, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len=seq_len)\n",
    "        self.encoder = TransformerEncoderBlock(d_model=d_model)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = self.input_proj(x)                    # (batch, seq_len, d_model)\n",
    "        x_proj = self.positional_encoding(x_proj)      # add positional info\n",
    "        encoded = self.encoder(x_proj)                 # (batch, seq_len, d_model)\n",
    "        decoded = self.decoder(encoded)                # (batch, seq_len, 1)\n",
    "        return decoded + x  # residual connection (only works if same shape)\n",
    "\n",
    "# ----------------------------------\n",
    "# Model Setup\n",
    "# ----------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FluxTransformerAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ----------------------------------\n",
    "# Training Loop\n",
    "# ----------------------------------\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        x = batch[0].to(device)\n",
    "        output = model(x)\n",
    "        loss = criterion(output, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"✅ Epoch {epoch+1} Loss: {total_loss / len(loader):.6f}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Embedding Extraction\n",
    "# ----------------------------------\n",
    "print(\"\\n🔍 Extracting embeddings...\")\n",
    "model.eval()\n",
    "model = model.cpu()\n",
    "\n",
    "embedding_list = []\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "        x = batch[0]\n",
    "        x_proj = model.input_proj(x)\n",
    "        x_proj = model.positional_encoding(x_proj)\n",
    "        encoded = model.encoder(x_proj)\n",
    "        pooled = encoded.mean(dim=1).numpy()  # Global average pooling\n",
    "        embedding_list.append(pooled)\n",
    "        del x, x_proj, encoded, pooled\n",
    "        gc.collect()\n",
    "\n",
    "# Save final embeddings\n",
    "flux_embeddings = np.concatenate(embedding_list, axis=0)\n",
    "np.save(\"flux_embeddings.npy\", flux_embeddings)\n",
    "print(\"✅ flux_embeddings.npy saved:\", flux_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f931f373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1088, 2000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load(\"X_flux_aligned.npy\")\n",
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
